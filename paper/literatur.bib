 @article{Berglind_Wisniewski_2014, title={Fatigue Estimation Methods Comparison for Wind Turbine Control}, url={http://arxiv.org/abs/1411.3925}, abstractNote={Fatigue is a critical factor in structures as wind turbines exposed to harsh operating conditions, both in the design stage and control during their operation. In the present paper the most recognized approaches to estimate the damage caused by fatigue are discussed and compared, with special focus on their applicability for wind turbine control. The aim of this paper is to serve as a guide among the vast literature on fatigue and shed some light on the underlying relationships between these methods.}, note={arXiv: 1411.3925}, journal={arXiv:1411.3925 [math]}, author={Berglind, J. J. Barradas and Wisniewski, Rafael}, year={2014}, month={Nov} }
 @article{Burda_Edwards_Storkey_Klimov_2018, title={Exploration by Random Network Distillation}, url={http://arxiv.org/abs/1810.12894}, abstractNote={We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma’s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.}, note={arXiv: 1810.12894}, journal={arXiv:1810.12894 [cs, stat]}, author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg}, year={2018}, month={Oct} }
 @inproceedings{Kolter_Jackowski_Tedrake_2012, title={Design, analysis, and learning control of a fully actuated micro wind turbine}, ISBN={978-1-4577-1096-4}, url={http://ieeexplore.ieee.org/document/6315604/}, DOI={10.1109/ACC.2012.6315604}, abstractNote={Wind power represents one of the most promising sources of renewable energy, and improvements to wind turbine design and control can have a signiﬁcant impact on energy sustainability. In this paper we make two primary contributions: ﬁrst, we develop and present a actuated micro wind turbine intended for research purposes. While most academic work on wind turbine control has largely focused on simulated evaluations, most turbine simulators are quite limited in their ability to model unsteady aerodynamic effects induced by the turbine; thus, there is a huge value to validating wind turbine control methods on a physical system, and the platform we present here makes this possible at a very low cost. The second contribution of this paper a novel policy search method, applied to optimizing power production in Region II wind speeds. Our method is similar in spirit to Reinforcement Learning approaches such as the REINFORCE algorithm, but explicitly models second order terms of the cost function and makes efﬁcient use of past execution data. We evaluate this method on the physical turbine and show it it is able to quickly and repeatably achieve near-optimal power production within about a minute of execution time and without any a priori model of the system.}, publisher={IEEE}, author={Kolter, J. Z. and Jackowski, Z. and Tedrake, R.}, year={2012}, month={Jun}, pages={2256–2263} }
 @article{Lillicrap_Hunt_Pritzel_Heess_Erez_Tassa_Silver_Wierstra_2015, title={CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING}, url={http://arxiv.org/abs/1509.02971}, abstractNote={We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.}, note={arXiv: 1509.02971}, journal={arXiv:1509.02971 [cs, stat]}, author={Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan}, year={2015}, month={Sep} }
 @article{Marten_Wendler_Pechlivanoglou_Nayeri_Paschereit_2013, title={QBLADE: AN OPEN SOURCE TOOL FOR DESIGN AND SIMULATION OF HORIZONTAL AND VERTICAL AXIS WIND TURBINES}, volume={3}, abstractNote={The software QBlade is developed as an open source framework for the simulation and design of wind turbines. QBlade utilizes the Blade Element Momentum (BEM) method for the simulation of horizontal axis- and a Double Multiple Streamtube (DMS) algorithm for the simulation of vertical axis wind turbine performance. For the design of custom airfoils and the computation of airfoil lift- and drag coefficient polars the viscous-inviscid coupled panel method code XFOIL is integrated within the graphical user interface of QBlade. Additionally, a module for the extrapolation of airfoil polars, beyond the stall point, for a 360° range of angles of attack is integrated. The resulting functionality allows the use of QBlade as a comprehensive tool for wind turbine design. QBlade is constantly being maintained, validated and advanced with new functionality. This paper describes the software and its modules, at the current state, in theory and application.}, number={3}, author={Marten, D and Wendler, J and Pechlivanoglou, G and Nayeri, C N and Paschereit, C O}, year={2013}, pages={6} }
 @article{Mnih_Kavukcuoglu_Silver_Graves_Antonoglou_Wierstra_Riedmiller_2013, title={Playing Atari with Deep Reinforcement Learning}, url={http://arxiv.org/abs/1312.5602}, abstractNote={We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}, note={arXiv: 1312.5602}, journal={arXiv:1312.5602 [cs]}, author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin}, year={2013}, month={Dec} }
 @article{Plappert_Houthooft_Dhariwal_Sidor_Chen_Chen_Asfour_Abbeel_Andrychowicz_2017, title={Parameter Space Noise for Exploration}, url={http://arxiv.org/abs/1706.01905}, abstractNote={Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent’s parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.}, note={arXiv: 1706.01905}, journal={arXiv:1706.01905 [cs, stat]}, author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin}, year={2017}, month={Jun} }
 @article{Silver_Lever_Heess_Degris_Wierstra_Riedmiller_2013, title={Deterministic Policy Gradient Algorithms}, abstractNote={In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efﬁciently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can signiﬁcantly outperform their stochastic counterparts in high-dimensional action spaces.}, author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin}, year={2013}, pages={9} }
 @book{Jonkman_Butterfield_Musial_Scott_2009, title={Definition of a 5-MW Reference Wind Turbine for Offshore System Development}, url={http://www.osti.gov/servlets/purl/947422-nhrlni/}, DOI={10.2172/947422}, number={NREL/TP-500-38060, 947422}, author={Jonkman, J. and Butterfield, S. and Musial, W. and Scott, G.}, year={2009}, month={Feb} }
 @book{Burton_Jenkins_Sharpe_Bossanyi_2011, place={Chichester, UK}, title={Wind Energy Handbook: Burton/Wind Energy Handbook}, ISBN={978-1-119-99271-4}, url={http://doi.wiley.com/10.1002/9781119992714}, DOI={10.1002/9781119992714}, publisher={John Wiley & Sons, Ltd}, author={Burton, Tony and Jenkins, Nick and Sharpe, David and Bossanyi, Ervin}, year={2011}, month={May} }
 @article{Howland_Lele_Dabiri_2019, title={Wind farm power optimization through wake steering}, volume={116}, ISSN={0027-8424, 1091-6490}, DOI={10.1073/pnas.1903680116}, number={29}, journal={Proceedings of the National Academy of Sciences}, author={Howland, Michael F. and Lele, Sanjiva K. and Dabiri, John O.}, year={2019}, month={Jul}, pages={14495–14500} }
 @article{Uhlenbeck_Ornstein_1930, title={On the Theory of the Brownian Motion}, volume={36}, ISSN={0031-899X}, DOI={10.1103/PhysRev.36.823}, number={5}, journal={Physical Review}, author={Uhlenbeck, G. E. and Ornstein, L. S.}, year={1930}, month={Sep}, pages={823–841} }
 @misc{rl-overview, url={https://medium.com/@jonathan_hui/rl-reinforcement-learning-algorithms-quick-overview-6bf69736694d} }
 @misc{envelope-detection, url={https://www.dsprelated.com/showarticle/938.php} }
 @misc{deep-rl-skymind, url={https://skymind.ai/wiki/deep-reinforcement-learning} }
 @misc{ddpg-explained-towardsdatascience, url={https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b} }
