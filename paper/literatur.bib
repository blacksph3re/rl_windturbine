
@online{DeepDeterministicPolicy,
  title = {Deep {{Deterministic Policy Gradients Explained}}},
  url = {https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b},
  journaltitle = {Towards Data Science}
}

@online{Rloverview,
  title = {Rl-Overview},
  url = {https://medium.com/@jonathan_hui/rl-reinforcement-learning-algorithms-quick-overview-6bf69736694d}
}

@online{Deeprlskymind,
  title = {Deep-Rl-Skymind},
  url = {https://skymind.ai/wiki/deep-reinforcement-learning}
}

@online{Envelopedetection,
  title = {Envelope-Detection},
  url = {https://www.dsprelated.com/showarticle/938.php}
}

@article{mnihPlayingAtariDeep2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5602},
  primaryClass = {cs},
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  urldate = {2019-10-17},
  date = {2013-12-19},
  keywords = {Computer Science - Machine Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  file = {/home/sph3re/Zotero/storage/TPZL5948/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/sph3re/Zotero/storage/ID3JK388/1312.html}
}

@article{lillicrapCONTINUOUSCONTROLDEEP2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02971},
  primaryClass = {cs, stat},
  title = {{{CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING}}},
  url = {http://arxiv.org/abs/1509.02971},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  urldate = {2019-10-17},
  date = {2015-09-09},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  file = {/home/sph3re/Zotero/storage/V3WB63Y8/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf;/home/sph3re/Zotero/storage/AJURCM2T/1509.html}
}

@article{silverDeterministicPolicyGradient2013,
  langid = {english},
  title = {Deterministic {{Policy Gradient Algorithms}}},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  date = {2013},
  pages = {9},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  file = {/home/sph3re/Programming/windturbine/paper/literature/deterministic policy gradient algorithms.pdf}
}

@article{berglindFatigueEstimationMethods2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.3925},
  primaryClass = {math},
  title = {Fatigue {{Estimation Methods Comparison}} for {{Wind Turbine Control}}},
  url = {http://arxiv.org/abs/1411.3925},
  abstract = {Fatigue is a critical factor in structures as wind turbines exposed to harsh operating conditions, both in the design stage and control during their operation. In the present paper the most recognized approaches to estimate the damage caused by fatigue are discussed and compared, with special focus on their applicability for wind turbine control. The aim of this paper is to serve as a guide among the vast literature on fatigue and shed some light on the underlying relationships between these methods.},
  urldate = {2019-10-17},
  date = {2014-11-14},
  keywords = {Mathematics - Optimization and Control},
  author = {Berglind, J. J. Barradas and Wisniewski, Rafael},
  file = {/home/sph3re/Zotero/storage/2L2BEFNB/Berglind and Wisniewski - 2014 - Fatigue Estimation Methods Comparison for Wind Tur.pdf;/home/sph3re/Zotero/storage/ZGKMSWX3/1411.html}
}

@article{martenQBLADEOPENSOURCE2013,
  langid = {english},
  title = {{{QBLADE}}: {{AN OPEN SOURCE TOOL FOR DESIGN AND SIMULATION OF HORIZONTAL AND VERTICAL AXIS WIND TURBINES}}},
  volume = {3},
  abstract = {The software QBlade is developed as an open source framework for the simulation and design of wind turbines. QBlade utilizes the Blade Element Momentum (BEM) method for the simulation of horizontal axis- and a Double Multiple Streamtube (DMS) algorithm for the simulation of vertical axis wind turbine performance. For the design of custom airfoils and the computation of airfoil lift- and drag coefficient polars the viscous-inviscid coupled panel method code XFOIL is integrated within the graphical user interface of QBlade. Additionally, a module for the extrapolation of airfoil polars, beyond the stall point, for a 360Â° range of angles of attack is integrated. The resulting functionality allows the use of QBlade as a comprehensive tool for wind turbine design. QBlade is constantly being maintained, validated and advanced with new functionality. This paper describes the software and its modules, at the current state, in theory and application.},
  number = {3},
  date = {2013},
  pages = {6},
  author = {Marten, D and Wendler, J and Pechlivanoglou, G and Nayeri, C N and Paschereit, C O},
  file = {/home/sph3re/Programming/windturbine/paper/literature/qblade.pdf}
}

@article{uhlenbeckTheoryBrownianMotion1930,
  langid = {english},
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  volume = {36},
  issn = {0031-899X},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
  doi = {10.1103/PhysRev.36.823},
  number = {5},
  journaltitle = {Physical Review},
  urldate = {2019-10-17},
  date = {1930-09-01},
  pages = {823-841},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.}
}

@article{plappertParameterSpaceNoise2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.01905},
  primaryClass = {cs, stat},
  title = {Parameter {{Space Noise}} for {{Exploration}}},
  url = {http://arxiv.org/abs/1706.01905},
  abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
  urldate = {2019-10-17},
  date = {2017-06-06},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  file = {/home/sph3re/Zotero/storage/PFV9YWWR/Plappert et al. - 2017 - Parameter Space Noise for Exploration.pdf;/home/sph3re/Zotero/storage/JPP3RHUA/1706.html}
}

@article{burdaExplorationRandomNetwork2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.12894},
  primaryClass = {cs, stat},
  title = {Exploration by {{Random Network Distillation}}},
  url = {http://arxiv.org/abs/1810.12894},
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  urldate = {2019-10-17},
  date = {2018-10-30},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  file = {/home/sph3re/Zotero/storage/YDU9U9SB/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf;/home/sph3re/Zotero/storage/PVH67VKQ/1810.html}
}

@inproceedings{kolterDesignAnalysisLearning2012,
  langid = {english},
  title = {Design, Analysis, and Learning Control of a Fully Actuated Micro Wind Turbine},
  isbn = {978-1-4577-1096-4 978-1-4577-1095-7 978-1-4577-1094-0 978-1-4673-2102-0},
  url = {http://ieeexplore.ieee.org/document/6315604/},
  doi = {10.1109/ACC.2012.6315604},
  abstract = {Wind power represents one of the most promising sources of renewable energy, and improvements to wind turbine design and control can have a significant impact on energy sustainability. In this paper we make two primary contributions: first, we develop and present a actuated micro wind turbine intended for research purposes. While most academic work on wind turbine control has largely focused on simulated evaluations, most turbine simulators are quite limited in their ability to model unsteady aerodynamic effects induced by the turbine; thus, there is a huge value to validating wind turbine control methods on a physical system, and the platform we present here makes this possible at a very low cost. The second contribution of this paper a novel policy search method, applied to optimizing power production in Region II wind speeds. Our method is similar in spirit to Reinforcement Learning approaches such as the REINFORCE algorithm, but explicitly models second order terms of the cost function and makes efficient use of past execution data. We evaluate this method on the physical turbine and show it it is able to quickly and repeatably achieve near-optimal power production within about a minute of execution time and without any a priori model of the system.},
  publisher = {{IEEE}},
  urldate = {2019-10-18},
  date = {2012-06},
  pages = {2256-2263},
  author = {Kolter, J. Z. and Jackowski, Z. and Tedrake, R.},
  file = {/home/sph3re/Programming/windturbine/paper/literature/kolter2012turbine.pdf}
}

@report{jonkmanDefinition5MWReference2009,
  langid = {english},
  title = {Definition of a 5-{{MW Reference Wind Turbine}} for {{Offshore System Development}}},
  url = {http://www.osti.gov/servlets/purl/947422-nhrlni/},
  number = {NREL/TP-500-38060, 947422},
  urldate = {2019-11-07},
  date = {2009-02-01},
  author = {Jonkman, J. and Butterfield, S. and Musial, W. and Scott, G.},
  file = {/home/sph3re/Programming/windturbine/paper/literature/38060.pdf},
  doi = {10.2172/947422}
}

@book{burtonWindEnergyHandbook2011,
  location = {{Chichester, UK}},
  title = {Wind {{Energy Handbook}}: {{Burton}}/{{Wind Energy Handbook}}},
  isbn = {978-1-119-99271-4 978-0-470-69975-1},
  url = {http://doi.wiley.com/10.1002/9781119992714},
  shorttitle = {Wind {{Energy Handbook}}},
  publisher = {{John Wiley \& Sons, Ltd}},
  urldate = {2019-11-07},
  date = {2011-05-06},
  author = {Burton, Tony and Jenkins, Nick and Sharpe, David and Bossanyi, Ervin},
  doi = {10.1002/9781119992714}
}

@article{howlandWindFarmPower2019,
  langid = {english},
  title = {Wind Farm Power Optimization through Wake Steering},
  volume = {116},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1903680116},
  doi = {10.1073/pnas.1903680116},
  number = {29},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2019-11-07},
  date = {2019-07-16},
  pages = {14495-14500},
  author = {Howland, Michael F. and Lele, Sanjiva K. and Dabiri, John O.},
  file = {/home/sph3re/Zotero/storage/S683QL64/Howland et al. - 2019 - Wind farm power optimization through wake steering.pdf}
}

@article{brockmanOpenAIGym2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.01540},
  primaryClass = {cs},
  title = {{{OpenAI Gym}}},
  url = {http://arxiv.org/abs/1606.01540},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  urldate = {2019-11-12},
  date = {2016-06-05},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  file = {/home/sph3re/Zotero/storage/BY4M98RR/Brockman et al. - 2016 - OpenAI Gym.pdf;/home/sph3re/Zotero/storage/JRXZ4TEN/1606.html}
}

@article{bellmanTheoryDynamicProgramming1954,
  langid = {english},
  title = {The Theory of Dynamic Programming},
  volume = {60},
  issn = {0002-9904},
  url = {http://www.ams.org/journal-getitem?pii=S0002-9904-1954-09848-8},
  doi = {10.1090/S0002-9904-1954-09848-8},
  number = {6},
  journaltitle = {Bulletin of the American Mathematical Society},
  urldate = {2019-11-12},
  date = {1954-11-01},
  pages = {503-516},
  author = {Bellman, Richard},
  file = {/home/sph3re/Programming/windturbine/paper/literature/euclid.bams.1183519147.pdf}
}

@article{fujimotoAddressingFunctionApproximation2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.09477},
  primaryClass = {cs, stat},
  title = {Addressing {{Function Approximation Error}} in {{Actor}}-{{Critic Methods}}},
  url = {http://arxiv.org/abs/1802.09477},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  urldate = {2019-11-12},
  date = {2018-10-22},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/BCAV2U9Y/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/home/sph3re/Zotero/storage/PBUKJ7KK/1802.html}
}


@article{hesselRainbowCombiningImprovements2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.02298},
  primaryClass = {cs},
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1710.02298},
  shorttitle = {Rainbow},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  urldate = {2019-11-12},
  date = {2017-10-06},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/PHQKYVHS/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/home/sph3re/Zotero/storage/VGSSU83J/1710.html}
}

@article{mozerDiscoveringStructureReactive1990,
  langid = {english},
  title = {Discovering the {{Structure}} of a {{Reactive Environment}} by {{Exploration}}},
  volume = {2},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1990.2.4.447},
  doi = {10.1162/neco.1990.2.4.447},
  number = {4},
  journaltitle = {Neural Computation},
  urldate = {2019-11-12},
  date = {1990-12},
  pages = {447-457},
  author = {Mozer, Michael C. and Bachrach, Jonathan}
}

@book{bishopNeuralNetworksPattern1995,
  location = {{Oxford : New York}},
  title = {Neural Networks for Pattern Recognition},
  isbn = {978-0-19-853849-3 978-0-19-853864-6},
  pagetotal = {482},
  publisher = {{Clarendon Press ; Oxford University Press}},
  date = {1995},
  keywords = {Neural networks (Computer science),Pattern recognition systems},
  author = {Bishop, Christopher M.}
}

@article{huber1964,
  title = {Robust Estimation of a Location Parameter},
  volume = {35},
  url = {https://doi.org/10.1214/aoms/1177703732},
  doi = {10.1214/aoms/1177703732},
  number = {1},
  journaltitle = {Ann. Math. Statist.},
  date = {1964-03},
  pages = {73-101},
  author = {Huber, Peter J.},
  fjournal = {The Annals of Mathematical Statistics},
  publisher = {{The Institute of Mathematical Statistics}}
}

@article{williamsSimpleStatisticalGradientfollowing,
  langid = {english},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  abstract = {This article presents a general class of associativereinforcementlearning algorithmsfor connectionist networkscontainingstochasticunits. These algorithms,calledREINFORCEalgorithms,are shownto makeweight adjustmentsin a direction that lies alongthe gradient of expectedreinforcementin both immediate-reinforcement tasks and certain limited forms of delayed-reinforcementtasks, and they do this without explicitly computing gradient estimatesor even storing informationfrom which such estimates couldbe computed. Specificexamples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novelbut potentiallyinterestingin their own right. Also givenare resultsthat showhow such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerationsthat might be used to help developsimilar but potentially more powerfulreinforcement learning algorithms.},
  pages = {28},
  author = {Williams, Ronald J},
  file = {/home/sph3re/Programming/windturbine/paper/literature/Williams1992_Article_SimpleStatisticalGradient-foll.pdf}
}

