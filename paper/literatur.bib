
@online{DeepDeterministicPolicy,
  title = {Deep {{Deterministic Policy Gradients Explained}}},
  url = {https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b},
  journaltitle = {Towards Data Science}
}

@online{Rloverview,
  title = {Rl-Overview},
  url = {https://medium.com/@jonathan_hui/rl-reinforcement-learning-algorithms-quick-overview-6bf69736694d}
}

@online{Deeprlskymind,
  title = {Deep-Rl-Skymind},
  url = {https://skymind.ai/wiki/deep-reinforcement-learning}
}

@online{Envelopedetection,
  title = {Envelope-Detection},
  url = {https://www.dsprelated.com/showarticle/938.php}
}

@article{mnihPlayingAtariDeep2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5602},
  primaryClass = {cs},
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  urldate = {2019-10-17},
  date = {2013-12-19},
  keywords = {Computer Science - Machine Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  file = {/home/sph3re/Zotero/storage/TPZL5948/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/sph3re/Zotero/storage/ID3JK388/1312.html}
}

@article{lillicrapCONTINUOUSCONTROLDEEP2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02971},
  primaryClass = {cs, stat},
  title = {{{CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING}}},
  url = {http://arxiv.org/abs/1509.02971},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  urldate = {2019-10-17},
  date = {2015-09-09},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  file = {/home/sph3re/Zotero/storage/V3WB63Y8/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf;/home/sph3re/Zotero/storage/AJURCM2T/1509.html}
}

@article{silverDeterministicPolicyGradient2013,
  langid = {english},
  title = {Deterministic {{Policy Gradient Algorithms}}},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  date = {2013},
  pages = {9},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  file = {/home/sph3re/Programming/windturbine/paper/literature/deterministic policy gradient algorithms.pdf}
}

@article{berglindFatigueEstimationMethods2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.3925},
  primaryClass = {math},
  title = {Fatigue {{Estimation Methods Comparison}} for {{Wind Turbine Control}}},
  url = {http://arxiv.org/abs/1411.3925},
  abstract = {Fatigue is a critical factor in structures as wind turbines exposed to harsh operating conditions, both in the design stage and control during their operation. In the present paper the most recognized approaches to estimate the damage caused by fatigue are discussed and compared, with special focus on their applicability for wind turbine control. The aim of this paper is to serve as a guide among the vast literature on fatigue and shed some light on the underlying relationships between these methods.},
  urldate = {2019-10-17},
  date = {2014-11-14},
  keywords = {Mathematics - Optimization and Control},
  author = {Berglind, J. J. Barradas and Wisniewski, Rafael},
  file = {/home/sph3re/Zotero/storage/2L2BEFNB/Berglind and Wisniewski - 2014 - Fatigue Estimation Methods Comparison for Wind Tur.pdf;/home/sph3re/Zotero/storage/ZGKMSWX3/1411.html}
}

@article{martenQBLADEOPENSOURCE2013,
  langid = {english},
  title = {{{QBLADE}}: {{AN OPEN SOURCE TOOL FOR DESIGN AND SIMULATION OF HORIZONTAL AND VERTICAL AXIS WIND TURBINES}}},
  volume = {3},
  abstract = {The software QBlade is developed as an open source framework for the simulation and design of wind turbines. QBlade utilizes the Blade Element Momentum (BEM) method for the simulation of horizontal axis- and a Double Multiple Streamtube (DMS) algorithm for the simulation of vertical axis wind turbine performance. For the design of custom airfoils and the computation of airfoil lift- and drag coefficient polars the viscous-inviscid coupled panel method code XFOIL is integrated within the graphical user interface of QBlade. Additionally, a module for the extrapolation of airfoil polars, beyond the stall point, for a 360Â° range of angles of attack is integrated. The resulting functionality allows the use of QBlade as a comprehensive tool for wind turbine design. QBlade is constantly being maintained, validated and advanced with new functionality. This paper describes the software and its modules, at the current state, in theory and application.},
  number = {3},
  date = {2013},
  pages = {6},
  author = {Marten, D and Wendler, J and Pechlivanoglou, G and Nayeri, C N and Paschereit, C O},
  file = {/home/sph3re/Programming/windturbine/paper/literature/qblade.pdf}
}

@article{uhlenbeckTheoryBrownianMotion1930,
  langid = {english},
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  volume = {36},
  issn = {0031-899X},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
  doi = {10.1103/PhysRev.36.823},
  number = {5},
  journaltitle = {Physical Review},
  urldate = {2019-10-17},
  date = {1930-09-01},
  pages = {823-841},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.}
}

@article{plappertParameterSpaceNoise2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.01905},
  primaryClass = {cs, stat},
  title = {Parameter {{Space Noise}} for {{Exploration}}},
  url = {http://arxiv.org/abs/1706.01905},
  abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
  urldate = {2019-10-17},
  date = {2017-06-06},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  file = {/home/sph3re/Zotero/storage/PFV9YWWR/Plappert et al. - 2017 - Parameter Space Noise for Exploration.pdf;/home/sph3re/Zotero/storage/JPP3RHUA/1706.html}
}

@article{burdaExplorationRandomNetwork2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.12894},
  primaryClass = {cs, stat},
  title = {Exploration by {{Random Network Distillation}}},
  url = {http://arxiv.org/abs/1810.12894},
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  urldate = {2019-10-17},
  date = {2018-10-30},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  file = {/home/sph3re/Zotero/storage/YDU9U9SB/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf;/home/sph3re/Zotero/storage/PVH67VKQ/1810.html}
}

@inproceedings{kolterDesignAnalysisLearning2012,
  langid = {english},
  title = {Design, Analysis, and Learning Control of a Fully Actuated Micro Wind Turbine},
  isbn = {978-1-4577-1096-4 978-1-4577-1095-7 978-1-4577-1094-0 978-1-4673-2102-0},
  url = {http://ieeexplore.ieee.org/document/6315604/},
  doi = {10.1109/ACC.2012.6315604},
  abstract = {Wind power represents one of the most promising sources of renewable energy, and improvements to wind turbine design and control can have a significant impact on energy sustainability. In this paper we make two primary contributions: first, we develop and present a actuated micro wind turbine intended for research purposes. While most academic work on wind turbine control has largely focused on simulated evaluations, most turbine simulators are quite limited in their ability to model unsteady aerodynamic effects induced by the turbine; thus, there is a huge value to validating wind turbine control methods on a physical system, and the platform we present here makes this possible at a very low cost. The second contribution of this paper a novel policy search method, applied to optimizing power production in Region II wind speeds. Our method is similar in spirit to Reinforcement Learning approaches such as the REINFORCE algorithm, but explicitly models second order terms of the cost function and makes efficient use of past execution data. We evaluate this method on the physical turbine and show it it is able to quickly and repeatably achieve near-optimal power production within about a minute of execution time and without any a priori model of the system.},
  publisher = {{IEEE}},
  urldate = {2019-10-18},
  date = {2012-06},
  pages = {2256-2263},
  author = {Kolter, J. Z. and Jackowski, Z. and Tedrake, R.},
  file = {/home/sph3re/Programming/windturbine/paper/literature/kolter2012turbine.pdf}
}

@report{jonkmanDefinition5MWReference2009,
  langid = {english},
  title = {Definition of a 5-{{MW Reference Wind Turbine}} for {{Offshore System Development}}},
  url = {http://www.osti.gov/servlets/purl/947422-nhrlni/},
  number = {NREL/TP-500-38060, 947422},
  urldate = {2019-11-07},
  date = {2009-02-01},
  author = {Jonkman, J. and Butterfield, S. and Musial, W. and Scott, G.},
  file = {/home/sph3re/Programming/windturbine/paper/literature/38060.pdf},
  doi = {10.2172/947422}
}

@book{burtonWindEnergyHandbook2011,
  location = {{Chichester, UK}},
  title = {Wind {{Energy Handbook}}: {{Burton}}/{{Wind Energy Handbook}}},
  isbn = {978-1-119-99271-4 978-0-470-69975-1},
  url = {http://doi.wiley.com/10.1002/9781119992714},
  shorttitle = {Wind {{Energy Handbook}}},
  publisher = {{John Wiley \& Sons, Ltd}},
  urldate = {2019-11-07},
  date = {2011-05-06},
  author = {Burton, Tony and Jenkins, Nick and Sharpe, David and Bossanyi, Ervin},
  doi = {10.1002/9781119992714}
}

@article{howlandWindFarmPower2019,
  langid = {english},
  title = {Wind Farm Power Optimization through Wake Steering},
  volume = {116},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1903680116},
  doi = {10.1073/pnas.1903680116},
  number = {29},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2019-11-07},
  date = {2019-07-16},
  pages = {14495-14500},
  author = {Howland, Michael F. and Lele, Sanjiva K. and Dabiri, John O.},
  file = {/home/sph3re/Zotero/storage/S683QL64/Howland et al. - 2019 - Wind farm power optimization through wake steering.pdf}
}

@article{brockmanOpenAIGym2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.01540},
  primaryClass = {cs},
  title = {{{OpenAI Gym}}},
  url = {http://arxiv.org/abs/1606.01540},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  urldate = {2019-11-12},
  date = {2016-06-05},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  file = {/home/sph3re/Zotero/storage/BY4M98RR/Brockman et al. - 2016 - OpenAI Gym.pdf;/home/sph3re/Zotero/storage/JRXZ4TEN/1606.html}
}

@article{bellmanTheoryDynamicProgramming1954,
  langid = {english},
  title = {The Theory of Dynamic Programming},
  volume = {60},
  issn = {0002-9904},
  url = {http://www.ams.org/journal-getitem?pii=S0002-9904-1954-09848-8},
  doi = {10.1090/S0002-9904-1954-09848-8},
  number = {6},
  journaltitle = {Bulletin of the American Mathematical Society},
  urldate = {2019-11-12},
  date = {1954-11-01},
  pages = {503-516},
  author = {Bellman, Richard},
  file = {/home/sph3re/Programming/windturbine/paper/literature/euclid.bams.1183519147.pdf}
}

@article{fujimotoAddressingFunctionApproximation2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.09477},
  primaryClass = {cs, stat},
  title = {Addressing {{Function Approximation Error}} in {{Actor}}-{{Critic Methods}}},
  url = {http://arxiv.org/abs/1802.09477},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  urldate = {2019-11-12},
  date = {2018-10-22},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/BCAV2U9Y/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/home/sph3re/Zotero/storage/PBUKJ7KK/1802.html}
}


@article{hesselRainbowCombiningImprovements2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.02298},
  primaryClass = {cs},
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1710.02298},
  shorttitle = {Rainbow},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  urldate = {2019-11-12},
  date = {2017-10-06},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/PHQKYVHS/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/home/sph3re/Zotero/storage/VGSSU83J/1710.html}
}

@article{mozerDiscoveringStructureReactive1990,
  langid = {english},
  title = {Discovering the {{Structure}} of a {{Reactive Environment}} by {{Exploration}}},
  volume = {2},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1990.2.4.447},
  doi = {10.1162/neco.1990.2.4.447},
  number = {4},
  journaltitle = {Neural Computation},
  urldate = {2019-11-12},
  date = {1990-12},
  pages = {447-457},
  author = {Mozer, Michael C. and Bachrach, Jonathan}
}

@book{bishopNeuralNetworksPattern1995,
  location = {{Oxford : New York}},
  title = {Neural Networks for Pattern Recognition},
  isbn = {978-0-19-853849-3 978-0-19-853864-6},
  pagetotal = {482},
  publisher = {{Clarendon Press ; Oxford University Press}},
  date = {1995},
  keywords = {Neural networks (Computer science),Pattern recognition systems},
  author = {Bishop, Christopher M.}
}

@article{cobbeQuantifyingGeneralizationReinforcement2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.02341},
  primaryClass = {cs, stat},
  title = {Quantifying {{Generalization}} in {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1812.02341},
  abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  urldate = {2019-11-12},
  date = {2019-07-14},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  file = {/home/sph3re/Zotero/storage/6JPS6GSY/Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf;/home/sph3re/Zotero/storage/MJ8QVC3I/1812.html}
}

@article{huber1964,
  title = {Robust Estimation of a Location Parameter},
  volume = {35},
  url = {https://doi.org/10.1214/aoms/1177703732},
  doi = {10.1214/aoms/1177703732},
  number = {1},
  journaltitle = {Ann. Math. Statist.},
  date = {1964-03},
  pages = {73-101},
  author = {Huber, Peter J.},
  fjournal = {The Annals of Mathematical Statistics},
  publisher = {{The Institute of Mathematical Statistics}}
}

@article{williamsSimpleStatisticalGradientfollowing,
  langid = {english},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  abstract = {This article presents a general class of associativereinforcementlearning algorithmsfor connectionist networkscontainingstochasticunits. These algorithms,calledREINFORCEalgorithms,are shownto makeweight adjustmentsin a direction that lies alongthe gradient of expectedreinforcementin both immediate-reinforcement tasks and certain limited forms of delayed-reinforcementtasks, and they do this without explicitly computing gradient estimatesor even storing informationfrom which such estimates couldbe computed. Specificexamples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novelbut potentiallyinterestingin their own right. Also givenare resultsthat showhow such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerationsthat might be used to help developsimilar but potentially more powerfulreinforcement learning algorithms.},
  pages = {28},
  author = {Williams, Ronald J},
  file = {/home/sph3re/Programming/windturbine/paper/literature/Williams1992_Article_SimpleStatisticalGradient-foll.pdf}
}

@article{bjorckUnderstandingBatchNormalization,
  langid = {english},
  title = {Understanding {{Batch Normalization}}},
  abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
  pages = {12},
  author = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  file = {/home/sph3re/Programming/windturbine/paper/literature/7996-understanding-batch-normalization.pdf}
}

@article{suttonPolicyGradientMethods,
  langid = {english},
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  pages = {7},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  file = {/home/sph3re/Programming/windturbine/paper/literature/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}
}


@article{degrisOffPolicyActorCritic2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1205.4839},
  primaryClass = {cs},
  title = {Off-{{Policy Actor}}-{{Critic}}},
  url = {http://arxiv.org/abs/1205.4839},
  abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
  urldate = {2019-11-28},
  date = {2013-06-20},
  keywords = {Computer Science - Machine Learning},
  author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
  file = {/home/sph3re/Zotero/storage/M8JKGVI7/Degris et al. - 2013 - Off-Policy Actor-Critic.pdf;/home/sph3re/Zotero/storage/BMB4EL4P/1205.html}
}

@article{schaulPrioritizedExperienceReplay2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05952},
  primaryClass = {cs},
  title = {Prioritized {{Experience Replay}}},
  url = {http://arxiv.org/abs/1511.05952},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  urldate = {2019-11-29},
  date = {2016-02-25},
  keywords = {Computer Science - Machine Learning},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  file = {/home/sph3re/Zotero/storage/Z8CT7GZT/Schaul et al. - 2016 - Prioritized Experience Replay.pdf}
}

@article{bhatnagarConvergentTemporalDifferenceLearning2009,
  langid = {english},
  title = {Convergent {{Temporal}}-{{Difference Learning}} with {{Arbitrary Smooth Function Approximation}}},
  abstract = {We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(Î»), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithmsâ€™ effectiveness.},
  date = {2009-12},
  pages = {9},
  author = {Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S and Maei, Hamid R and SzepesvÃ¡ri, Csaba},
  file = {/home/sph3re/Programming/windturbine/paper/literature/3809-convergent-temporal-difference-learning-with-arbitrary-smooth-function-approximation.pdf}
}


@article{kingmaAdamMethodStochastic2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-12-02},
  date = {2017-01-29},
  keywords = {Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/home/sph3re/Zotero/storage/K8KPX9LC/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/sph3re/Zotero/storage/JQ7BB7T6/1412.html}
}

@article{agarapDeepLearningUsing2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08375},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}} Using {{Rectified Linear Units}} ({{ReLU}})},
  url = {http://arxiv.org/abs/1803.08375},
  abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer \$h\_\{n - 1\}\$ in a neural network, then multiply it by weight parameters \$\textbackslash{}theta\$ to get the raw scores \$o\_\{i\}\$. Afterwards, we threshold the raw scores \$o\_\{i\}\$ by \$0\$, i.e. \$f(o) = \textbackslash{}max(0, o\_\{i\})\$, where \$f(o)\$ is the ReLU function. We provide class predictions \$\textbackslash{}hat\{y\}\$ through argmax function, i.e. argmax \$f(x)\$.},
  urldate = {2019-12-02},
  date = {2019-02-07},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Computer Vision and Pattern Recognition},
  author = {Agarap, Abien Fred},
  file = {/home/sph3re/Zotero/storage/Q3GEUQBM/Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf;/home/sph3re/Zotero/storage/AQSEKQDV/1803.html}
}

@article{houImprovingDDPGPrioritized,
  langid = {english},
  title = {Improving {{DDPG}} via {{Prioritized Experience Replay}}},
  abstract = {Experience replay plays an important role in reinforcement learning. It reuses previous experiences to prevent the input data from being highly correlated. Recently, a deep reinforcement learning algorithm with experience replay, called deep deterministic policy gradient (DDPG), has achieved good performance in many continuous control tasks. However, it assumes the experiences are of equal importance and samples them uniformly, which obviously neglects the difference in the value of each individual experience. To improve the efficiency of experience replay in DDPG method, we propose to replace the original uniform experience replay with prioritized experience replay. We test the algorithms in five tasks in the OpenAI Gym, a testbed for reinforcement learning algorithms. In the experiment, we find that DDPG with prioritized experience replay mechanism significantly outperforms that with uniform sampling in terms of training time, training stability and final performance. We also find that our algorithm can achieve better performance in three tasks compared with some state-of-the-art algorithms like Q-prop, which directly proves the effectiveness of our proposed scheme. Our weekly report and codes are available at https://github.com/cardwing/Codes-for-RL-Project.},
  pages = {10},
  author = {Hou, Yuenan and Zhang, Yi},
  file = {/home/sph3re/Programming/windturbine/paper/literature/RL_course_report.pdf}
}

@article{zhaExperienceReplayOptimization2019,
  langid = {english},
  title = {Experience {{Replay Optimization}}},
  abstract = {Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rulebased replay strategy, which may be sub-optimal. In this work, we consider learning a replay policy to optimize the cumulative reward. Replay learning is challenging because the replay memory is noisy and large, and the cumulative reward is unstable. To address these issues, we propose a novel experience replay optimization (ERO) framework which alternately updates two policies: the agent policy, and the replay policy. The agent is updated to maximize the cumulative reward based on the replayed data, while the replay policy is updated to provide the agent with the most useful experiences. The conducted experiments on various continuous control tasks demonstrate the effectiveness of ERO, empirically showing promise in experience replay learning to improve the performance of off-policy reinforcement learning algorithms.},
  date = {2019-08},
  pages = {7},
  author = {Zha, Daochen and Lai, Kwei-Herng and Zhou, Kaixiong and Hu, Xia},
  file = {/home/sph3re/Programming/windturbine/paper/literature/0589.pdf}
}


@article{vanhasseltLearningValuesMany2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07714},
  primaryClass = {cs, stat},
  title = {Learning Values across Many Orders of Magnitude},
  url = {http://arxiv.org/abs/1602.07714},
  abstract = {Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
  urldate = {2019-12-03},
  date = {2016-08-16},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  author = {van Hasselt, Hado and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/9J6QURSP/van Hasselt et al. - 2016 - Learning values across many orders of magnitude.pdf;/home/sph3re/Zotero/storage/E8EFBV7W/1602.html}
}

@article{mnihHumanlevelControlDeep2015,
  langid = {english},
  title = {Human-Level Control through Deep Reinforcement Learning},
  volume = {518},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature14236},
  doi = {10.1038/nature14236},
  number = {7540},
  journaltitle = {Nature},
  urldate = {2019-12-03},
  date = {2015-02},
  pages = {529-533},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  file = {/home/sph3re/Programming/windturbine/paper/literature/nature14236.pdf}
}

@article{vankuikLongtermResearchChallenges2016,
  langid = {english},
  title = {Long-Term Research Challenges in Wind Energy“ a Research Agenda by the {{European Academy}} of {{Wind Energy}}},
  volume = {1},
  issn = {2366-7451},
  url = {http://www.wind-energ-sci.net/1/1/2016/},
  doi = {10.5194/wes-1-1-2016},
  abstract = {The European Academy of Wind Energy (eawe), representing universities and institutes with a significant wind energy programme in 14 countries, has discussed the long-term research challenges in wind energy. In contrast to research agendas addressing short- to medium-term research activities, this eawe document takes a longer-term perspective, addressing the scientific knowledge base that is required to develop wind energy beyond the applications of today and tomorrow. In other words, this long-term research agenda is driven by problems and curiosity, addressing basic research and fundamental knowledge in 11 research areas, ranging from physics and design to environmental and societal aspects. Because of the very nature of this initiative, this document does not intend to be permanent or complete. It shows the vision of the experts of the eawe, but other views may be possible. We sincerely hope that it will spur an even more intensive discussion worldwide within the wind energy community.},
  number = {1},
  journaltitle = {Wind Energy Science},
  urldate = {2019-12-04},
  date = {2016-02-09},
  pages = {1-39},
  author = {van Kuik, G. A. M. and Peinke, J. and Nijssen, R. and Lekou, D. and Mann, J. and SA¸rensen, J. N. and Ferreira, C. and van Wingerden, J. W. and Schlipf, D. and Gebraad, P. and Polinder, H. and Abrahamsen, A. and van Bussel, G. J. W. and SA¸rensen, J. D. and Tavner, P. and Bottasso, C. L. and Muskulus, M. and Matha, D. and Lindeboom, H. J. and Degraer, S. and Kramer, O. and Lehnhoff, S. and Sonnenschein, M. and SA¸rensen, P. E. and Konneke, R. W. and Morthorst, P. E. and Skytte, K.},
  options = {useprefix=true},
  file = {/home/sph3re/Programming/windturbine/paper/literature/wes-1-1-2016.pdf}
}

@article{silverMasteringGameGo2016,
  langid = {english},
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  volume = {529},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature16961},
  doi = {10.1038/nature16961},
  number = {7587},
  journaltitle = {Nature},
  urldate = {2019-12-04},
  date = {2016-01},
  pages = {484-489},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  file = {/home/sph3re/Programming/windturbine/paper/literature/nature16961.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  langid = {english},
  location = {{Cambridge, Massachusetts}},
  title = {Reinforcement Learning: An Introduction},
  edition = {Second edition},
  isbn = {978-0-262-03924-6},
  shorttitle = {Reinforcement Learning},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  pagetotal = {526},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{The MIT Press}},
  date = {2018},
  keywords = {Reinforcement learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {/home/sph3re/Programming/windturbine/paper/literature/RLbook2018.pdf}
}

@article{schulmanTrustRegionPolicy2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05477},
  primaryClass = {cs},
  title = {Trust {{Region Policy Optimization}}},
  url = {http://arxiv.org/abs/1502.05477},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  urldate = {2019-12-04},
  date = {2017-04-20},
  keywords = {Computer Science - Machine Learning},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  file = {/home/sph3re/Zotero/storage/8ZD6H3CL/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/home/sph3re/Zotero/storage/F5II5PR4/1502.html}
}

@article{vanhasseltDeepReinforcementLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06461},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  url = {http://arxiv.org/abs/1509.06461},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  urldate = {2019-12-05},
  date = {2015-12-08},
  keywords = {Computer Science - Machine Learning},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  options = {useprefix=true},
  file = {/home/sph3re/Zotero/storage/9U7KI379/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf;/home/sph3re/Zotero/storage/JXZEFBYM/1509.html}
}

