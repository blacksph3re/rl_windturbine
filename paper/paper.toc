\contentsline {section}{\numberline {0.1}Background:}{2}{section.0.1}% 
\contentsline {section}{\numberline {0.2}Tasks}{2}{section.0.2}% 
\contentsline {chapter}{\numberline {1}Abstract}{3}{chapter.1}% 
\contentsline {chapter}{\numberline {2}Intro}{4}{chapter.2}% 
\contentsline {chapter}{\numberline {3}Background}{5}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Windturbine control}{5}{section.3.1}% 
\contentsline {section}{\numberline {3.2}QBlade}{6}{section.3.2}% 
\contentsline {section}{\numberline {3.3}Reinforcement learning}{8}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Environment assumptions}{8}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Definitions}{9}{subsection.3.3.2}% 
\contentsline {subsection}{\numberline {3.3.3}Q-Learning}{10}{subsection.3.3.3}% 
\contentsline {subsection}{\numberline {3.3.4}PG}{11}{subsection.3.3.4}% 
\contentsline {subsection}{\numberline {3.3.5}DDPG}{12}{subsection.3.3.5}% 
\contentsline {section}{\numberline {3.4}RL on windturbines}{12}{section.3.4}% 
\contentsline {chapter}{\numberline {4}Early experimentation}{13}{chapter.4}% 
\contentsline {chapter}{\numberline {5}Experimentation}{14}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Designing reward functions}{14}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Aiding exploration}{16}{section.5.2}% 
\contentsline {subsection}{\numberline {5.2.1}Action Noise}{16}{subsection.5.2.1}% 
\contentsline {subsection}{\numberline {5.2.2}Random exploration}{17}{subsection.5.2.2}% 
\contentsline {subsection}{\numberline {5.2.3}Parameter noise}{17}{subsection.5.2.3}% 
\contentsline {section}{\numberline {5.3}Zero Gradients}{17}{section.5.3}% 
\contentsline {subsection}{\numberline {5.3.1}Simplifying the architecture}{17}{subsection.5.3.1}% 
\contentsline {subsection}{\numberline {5.3.2}Normalization}{18}{subsection.5.3.2}% 
\contentsline {section}{\numberline {5.4}High action gradients}{18}{section.5.4}% 
\contentsline {subsection}{\numberline {5.4.1}Gradient actionspace}{18}{subsection.5.4.1}% 
\contentsline {subsection}{\numberline {5.4.2}Feeding past timesteps}{19}{subsection.5.4.2}% 
\contentsline {subsection}{\numberline {5.4.3}Clipping action gradients}{19}{subsection.5.4.3}% 
\contentsline {subsection}{\numberline {5.4.4}Pretraining the policy}{19}{subsection.5.4.4}% 
\contentsline {section}{\numberline {5.5}Improvements without sense}{19}{section.5.5}% 
\contentsline {subsection}{\numberline {5.5.1}Prioritized experience replay}{19}{subsection.5.5.1}% 
\contentsline {subsection}{\numberline {5.5.2}Data augmentation}{19}{subsection.5.5.2}% 
\contentsline {section}{\numberline {5.6}Exploding Q-Loss}{20}{section.5.6}% 
\contentsline {subsection}{\numberline {5.6.1}Huber loss}{20}{subsection.5.6.1}% 
\contentsline {subsection}{\numberline {5.6.2}Double critic}{20}{subsection.5.6.2}% 
\contentsline {subsection}{\numberline {5.6.3}Batch normalization}{20}{subsection.5.6.3}% 
\contentsline {section}{\numberline {5.7}Automatized hyperparameter search}{20}{section.5.7}% 
\contentsline {chapter}{\numberline {6}Evaluation}{21}{chapter.6}% 
\contentsline {section}{\numberline {6.1}First working version}{21}{section.6.1}% 
\contentsline {section}{\numberline {6.2}HParam tuning}{21}{section.6.2}% 
\contentsline {section}{\numberline {6.3}Comparison to industry controllers}{22}{section.6.3}% 
\contentsline {chapter}{\numberline {7}Future work}{23}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Reward functions}{23}{section.7.1}% 
\contentsline {section}{\numberline {7.2}Switch reward functions during training}{23}{section.7.2}% 
\contentsline {section}{\numberline {7.3}Expert policy training}{23}{section.7.3}% 
\contentsline {section}{\numberline {7.4}Expert policy in instable conditions}{24}{section.7.4}% 
\contentsline {section}{\numberline {7.5}Active control elements}{24}{section.7.5}% 
\contentsline {chapter}{\numberline {8}Conclusion}{25}{chapter.8}% 
\contentsline {chapter}{\nonumberline Literaturverzeichnis}{26}{chapter*.4}% 
